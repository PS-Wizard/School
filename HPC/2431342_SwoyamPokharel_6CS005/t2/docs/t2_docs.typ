#import "layout.typ": *

#show: academic-theme.with(
  title: "Implementation Of A Multithreaded Matrix Operations Program",
  author: "Swoyam Pokharel",
)

#pagebreak()

==== SUMMARY

```bash
.
├── assets
│   ├── MatData.txt
│   └── Matrices.txt
├── docs
│   ├── images
│   │   ├── herald.png
│   │   ├── screenshots
│   │   │   ├── Screenshot from 2025-12-19 09-01-08.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-01.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-08.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-10.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-12.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-14.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-19.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-22.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-32.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-35.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-39.png
│   │   │   ├── Screenshot from 2025-12-19 09-02-43.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-25.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-27.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-28.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-29.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-30.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-31.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-32.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-36.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-39.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-40.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-45.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-46.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-47.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-48.png
│   │   │   ├── Screenshot from 2026-01-10 07-57-49.png
│   │   │   └── Screenshot from 2026-01-10 07-57-53.png
│   │   └── wolverhampton.png
│   ├── layout.typ
│   ├── README.md
│   ├── t2_docs.pdf
│   └── t2_docs.typ
├── outputs
│   ├── MatData.txt
│   ├── Matrices.txt
│   ├── results_MatData.txt
│   └── results_Matrices.txt
├── README.pdf
└── src
    ├── file_ops.c
    ├── file_ops.h
    ├── main.c
    ├── Makefile
    ├── matrix_ops.c
    ├── matrix_ops.h
    └── matrix_program

7 directories, 48 files
```
- *`docs/`* contains the #link("https://typst.app/")[typst] source that was used to generate this `README.pdf` along with screenshots and images used in this pdf.
- *`outputs/`* contains the `results_MatData.txt` & `results_Matrices.txt` generated by the program using the input datasets `MatData.txt` & `Matrices.txt` 
- *`assets/`* contains the provided input datasets.
- *`src/`* contains the actual source code

#pagebreak()
= Program Architecture
The program is split into three main modules, each handling a distinct part of the application. The overall logic stays the same as the code that was provided to us, but it has been improved error handling, modularization, and implemented the functionality the problem statement asks for.

== Module Structure

The codebase consists of 5 files organized into three parts:

- *main.c*: Entry point and command-line interface
- *matrix_ops.c/h*: Matrix operations and memory management
- *file_ops.c/h*: File I/O and processing coordination

== Main Module

The main module is the program's entry point, handling command-line argument validation, file opening, and calling the rest of the functions:

```c
int main(int argc, char* argv[]) {
    // Validate command line arguments
    if (argc < 3) {
        fprintf(stderr, "Usage: %s <input_file> <num_threads>\n", argv[0]);
        fprintf(stderr, "Example: %s matrices.txt 4\n", argv[0]);
        return 1;
    }

    // Open input file
    FILE* in = fopen(argv[1], "r");
    if (!in) {
        fprintf(stderr, "Error: Cannot open input file '%s'\n", argv[1]);
        return 1;
    }

    // Parse thread count
    int threads = atoi(argv[2]);
    if (threads <= 0) {
        fprintf(stderr, "Error: Number of threads must be positive (got %d)\n", threads);
        fclose(in);
        return 1;
    }

    // Open output file
    FILE* out = fopen("results.txt", "w");
    if (!out) {
        fprintf(stderr, "Error: Cannot create output file 'results.txt'\n");
        fclose(in);
        return 1;
    }

    // Process all matrix pairs
    int pairCount = processMatrixPairs(in, out, threads);

    // Cleanup
    fclose(in);
    fclose(out);

    // Report results
    printf("Processing complete. Results written to results.txt\n");
    printf("Processed %d matrix pair(s)\n", pairCount);

    return 0;
}
```

=== Argument Validation

The program needs exactly two command line arguments: an input filename and a thread count.

- The first check verifies argument count. The `argc` variable includes the program name itself as `argv[0]`, so we need `argc >= 3` to have both required arguments.

- The second validation parses the thread count using `atoi`, which converts a string to an integer, returning 0 if the string isn't valid.

- The check `threads <= 0` catches both explicit zero values and invalid inputs like "abc" or an empty string.

After all validations pass and files are open, the main function calls `processMatrixPairs`, passing the file handles and thread count. This processes all matrix pairs found in the input file, returning the count of pairs successfully processed. The main function just coordinates the high-level flow: validate inputs, open resources, call processing, close resources, report results.

= File Operations Module

The file operations module handles all interaction with the file system - reading matrix data from the input file, validating parsed data, and writing computed results to the output file.

== Reading Matrices from Input File

The input file contains multiple matrices in a specific format, with each matrix preceded by dimension information.

=== Input Format Structure

Each matrix in the file follows this pattern:
```
rows,cols
value1,value2,value3,...
value4,value5,value6,...
...
```

The dimensions appear on a single line as two integers separated by a comma, followed by all matrix elements. Matrices are processed as pairs: 
- the first matrix read becomes matrix A, 
- the second becomes matrix B, 
- then the third becomes the next pair's A, and so on.

Each line and each value is terminated with a distinct character (newline for rows, comma for values), making it straightforward to parse with `fscanf`.

=== The Processing Loop

The core processing function `processMatrixPairs` implements a loop that reads and processes matrix pairs until the input file is exhausted:

```c
int processMatrixPairs(FILE* in, FILE* out, int threads) {
    int pairNum = 0;

    while (1) {
        int r1, c1, r2, c2;

        // Try to read first matrix dimensions
        int result = fscanf(in, "%d,%d", &r1, &c1);
        if (result == EOF) {
            break;
        }

        if (result != 2) {
            fprintf(stderr, "Error: Invalid header format for matrix A in pair %d\n", pairNum + 1);
            fprintf(stderr, "Expected format: rows,cols\n");
            break;
        }

        if (r1 <= 0 || c1 <= 0) {
            fprintf(stderr, "Error: Invalid dimensions %d,%d for matrix A in pair %d\n", r1, c1, pairNum + 1);
            break;
        }

        double** A = allocMatrix(r1, c1);
        if (!A) {
            fprintf(stderr, "Error: Failed to allocate matrix A (%d x %d) in pair %d\n", r1, c1, pairNum + 1);
            break;
        }

        if (!readMatrix(in, A, r1, c1, "A")) {
            freeMatrix(A, r1);
            break;
        }

        // Try to read second matrix dimensions
        result = fscanf(in, "%d,%d", &r2, &c2);
        if (result != 2) {
            fprintf(stderr, "Error: Invalid header format for matrix B in pair %d\n", pairNum + 1);
            freeMatrix(A, r1);
            break;
        }

        if (r2 <= 0 || c2 <= 0) {
            fprintf(stderr, "Error: Invalid dimensions %d,%d for matrix B in pair %d\n", r2, c2, pairNum + 1);
            freeMatrix(A, r1);
            break;
        }

        double** B = allocMatrix(r2, c2);
        if (!B) {
            fprintf(stderr, "Error: Failed to allocate matrix B (%d x %d) in pair %d\n", r2, c2, pairNum + 1);
            freeMatrix(A, r1);
            break;
        }

        if (!readMatrix(in, B, r2, c2, "B")) {
            freeMatrix(A, r1);
            freeMatrix(B, r2);
            break;
        }

        pairNum++;
        performOperations(out, A, B, r1, c1, r2, c2, threads, pairNum);

        freeMatrix(A, r1);
        freeMatrix(B, r2);
    }

    if (pairNum == 0) {
        fprintf(out, "No valid matrix pairs found in input file.\n");
        fprintf(stderr, "Warning: No valid matrix pairs were processed\n");
    } else {
        fprintf(out, "Processed %d matrix pair(s) successfully.\n", pairNum);
    }

    return pairNum;
}
```

Each iteration attempts to read one complete matrix pair, performing all operations on that pair before moving to the next.

=== Dimension Parsing and Validation

Reading matrix dimensions is the first operation for each matrix. The `fscanf` function attempts to parse two integers separated by a comma:

```c
int result = fscanf(in, "%d,%d", &r1, &c1);
if (result == EOF) {
    break; // End of file reached
}
if (result != 2) {
    fprintf(stderr, "Error: Invalid header format for matrix A in pair %d\n",
            pairNum + 1);
    fprintf(stderr, "Expected format: rows,cols\n");
    break;
}
```

The return value from `fscanf` gives us how many values were successfully parsed. A return of `EOF` signals end of file, which is the normal termination condition for the processing loop. Any other return value that isn't exactly 2 indicates malformed input.

After successfully parsing dimensions, the program validates they're positive and reasonable:

```c
if (r1 <= 0 || c1 <= 0) {
    fprintf(stderr, "Error: Invalid dimensions %d,%d for matrix A in pair %d\n", r1, c1, pairNum + 1);
    break;
}
```

=== Matrix Data Reading

Once dimensions are validated and memory is allocated, the `readMatrix` function handles reading the actual matrix values:

```c
int readMatrix(FILE* in, double** matrix, int r, int c, const char* name) {
    int elementsRead = 0;

    for (int i = 0; i < r; i++) {
        for (int j = 0; j < c; j++) {
            if (fscanf(in, "%lf,", &matrix[i][j]) != 1) {
                fprintf(stderr, "Error: Failed to read element [%d][%d] of matrix %s\n",
                        i, j, name);
                fprintf(stderr, "Expected %d total elements, only read %d\n",
                        r * c, elementsRead);
                return 0;
            }
            elementsRead++;
        }
    }

    if (elementsRead != r * c) {
        fprintf(stderr, "Error: Matrix %s dimension mismatch. Expected %d elements, read %d\n",
                name, r * c, elementsRead);
        return 0;
    }

    return 1;
}
```

This function reads values in order, filling each row completely before moving to the next. The nested loops iterate through all expected positions, and `fscanf` attempts to parse a double followed by a comma at each position.

The format string `"%lf,"` tells `fscanf` to skip any leading whitespace, parse a floating-point number, and expect and consume a comma. This format matches the input file structure exactly, where every value ends with a comma.

The element counter provides an additional validation layer. Even if all individual `fscanf` calls succeed, comparing the final count against the expected total `r * c` catches issues like extra or missing values that might occur if the file format is wrong.

=== Error Recovery and Memory Cleanup

When parsing fails for any matrix, the program has to clean up any memory already allocated before terminating the processing loop:

```c
double** A = allocMatrix(r1, c1);
if (!A) {
    fprintf(stderr, "Error: Failed to allocate matrix A (%d x %d) in pair %d\n", r1, c1, pairNum + 1);
    break;
}

if (!readMatrix(in, A, r1, c1, "A")) {
    freeMatrix(A, r1);
    break;
}

// Similar for matrix B...
if (!readMatrix(in, B, r2, c2, "B")) {
    freeMatrix(A, r1);
    freeMatrix(B, r2);
    break;
}
```

This pattern appears throughout. If reading matrix A fails, we free A and break. If reading matrix B fails, we free both A and B before breaking. This ensures no memory leaks occur even when processing gets interrupted by malformed input.

== Writing Results to Output File

The output file format mirrors the input structure but includes clear labels and separators to make results easy to read. Each matrix pair gets its own section with all applicable operations performed and their results displayed.

=== Operation Execution and Output

The `performOperations` function coordinates all operations for a single matrix pair:

```c
void performOperations(FILE* out, double** A, double** B, int r1, int c1, int r2, int c2, int threads, int pairNum) {
    fprintf(out, "===============================\n");
    fprintf(out, "MATRIX PAIR %d\n", pairNum);
    fprintf(out, "Matrix A: %d x %d\n", r1, c1);
    fprintf(out, "Matrix B: %d x %d\n", r2, c2);
    fprintf(out, "===============================\n\n");

    // Element-wise operations (require same dimensions)
    if (r1 == r2 && c1 == c2) {
        fprintf(out, "Addition - %d,%d\n", r1, c1);
        double** R = add(A, B, r1, c1, threads);
        if (R) {
            printMatrix(out, R, r1, c1);
            freeMatrix(R, r1);
        } else {
            fprintf(out, "Error: Memory allocation failed\n");
        }

        // Similar for subtraction, element-wise multiply, element-wise divide
    } else {
        fprintf(out, "Addition cannot be done - different sizes\n");
        fprintf(out, "Subtraction cannot be done - different sizes\n");
        fprintf(out, "Element-wise Multiply cannot be done - different sizes\n");
        fprintf(out, "Element-wise Divide cannot be done - different sizes\n");
    }

    // Transpose operations (always valid)
    fprintf(out, "\nTranspose A - %d,%d\n", c1, r1);
    double** T = transpose(A, r1, c1, threads);
    if (T) {
        printMatrix(out, T, c1, r1);
        freeMatrix(T, c1);
    }

    // Matrix multiplication (requires c1 == r2)
    if (c1 == r2) {
        fprintf(out, "\nMatrix Multiply A x B - %d,%d\n", r1, c2);
        double** R = matMul(A, B, r1, c1, c2, threads);
        if (R) {
            printMatrix(out, R, r1, c2);
            freeMatrix(R, r1);
        }
    } else {
        fprintf(out, "\nMatrix Multiply cannot be done;  A columns (%d) != B rows (%d)\n", c1, r2);
    }
}
```

The function starts with a header showing the pair number and dimensions of both matrices. This header provides context for all subsequent operations, making it immediately clear which matrices are being processed.

Each operation follows the same pattern: 
- check if the operation is valid based on dimensions, attempt to perform it 
- if valid, check if the result matrix was successfully allocated, 
- print the result if successful, and free the result immediately after printing. 

This pattern ensures consistent handling across all operations and prevents memory leaks.

=== Output Format Compliance

The output format follows the question's specification. Each operation result begins with a line showing the operation name and result dimensions in the format `"Operation - rows,cols"`:

```c
fprintf(out, "Addition - %d,%d\n", r1, c1);
```

When an operation cannot be performed, a single-line message explains why:

```c
fprintf(out, "Addition cannot be done - different sizes\n");
```

=== Matrix Printing

The `printMatrix` function handles the actual output formatting:

```c
void printMatrix(FILE* f, double** M, int r, int c) {
    for (int i = 0; i < r; i++) {
        for (int j = 0; j < c; j++) {
            fprintf(f, "%lf", M[i][j]);
            if (j < c - 1) fprintf(f, ", ");
        }
        fprintf(f, "\n");
    }
}
```

This function iterates through rows and columns, printing each value with `%lf` format, which displays doubles with full precision. Commas separate values within a row, but the last value in each row gets no trailing comma. Each row ends with a newline, creating the matrix layout where each row appears on its own line.

= Matrix Operations Module

The matrix operations module implements all the computational functions for the matrices; memory management for matrices and the seven required matrix operations. Each operation is parallelized using OpenMP to distribute work across multiple threads.

== Memory Management

=== Dynamic Array Storage

The `allocMatrix` function allocates memory for a matrix with the specified dimensions:

```c
double** allocMatrix(int r, int c) {
    if (r <= 0 || c <= 0) {
        fprintf(stderr, "Error: Invalid matrix dimensions %d x %d\n", r, c);
        return NULL;
    }

    double** m = malloc(r * sizeof(double*));
    if (!m) {
        fprintf(stderr, "Error: Memory allocation failed for matrix rows\n");
        return NULL;
    }

    for (int i = 0; i < r; i++) {
        m[i] = malloc(c * sizeof(double));
        if (!m[i]) {
            fprintf(stderr, "Error: Memory allocation failed for matrix row %d\n", i);
            for (int j = 0; j < i; j++)
                free(m[j]);
            free(m);
            return NULL;
        }
    }
    return m;
}
```

The function starts by validating dimensions, rejecting zero or negative values that would represent errors. The validation occurs before any allocation attempts.

Memory allocation happens in two stages. First, we allocate an array of `r` pointers, where each pointer will point to a row. Then, we allocate `c` doubles for each row. If allocating the pointer array fails, we return `NULL` immediately. If allocating any individual row fails, we have to clean up all previously allocated rows before returning `NULL`. This cleanup happens in the failure path itself:

```c
for (int j = 0; j < i; j++)
    free(m[j]);
free(m);
return NULL;
```

=== Deallocation

The `freeMatrix` function deallocates a matrix in the reverse order of allocation:

```c
void freeMatrix(double** m, int r) {
    if (!m) return;
    for (int i = 0; i < r; i++)
        free(m[i]);
    free(m);
}
```

The null check at the beginning makes this function safe to call on failed allocations. If `allocMatrix` returns `NULL` due to an error, calling `freeMatrix` on that `NULL` pointer does nothing. 

Each row must be freed individually because each was allocated with its own `malloc` call. Only after freeing all rows can the row pointer array itself be freed. This matters because freeing the pointer array first would lose access to the row pointers, making it impossible to free the row memory and causing a memory leak.

== Thread Capping

One of the requirements were to cap the actual thread count to prevent using more threads than the problem can effectively parallelize. This optimization prevents wasteful thread creation when the workload is small relative to the requested thread count.

=== The Capping Function

```c
int capThreads(int requested, int row) {
    if (requested > row) return row;
    return requested;
}
```

- If the user requests more threads than there are rows, cap to the number of rows. For a 3x100 matrix, using more than 3 threads provides no benefit since the parallel loop only has 3 iterations to distribute.

=== Application in Operations

Each operation calls `capThreads` before creating the parallel region:

```c
int actualThreads = capThreads(threads, r);

#pragma omp parallel for num_threads(actualThreads)
// ... operation logic
```

== Matrix Operations

The program implements seven distinct matrix operations, each parallelized using OpenMP. The operations handle matrices of arbitrary size, with dimension checking determining which operations are valid for each pair.

=== Element-wise Operations

Element-wise operations require both input matrices to have identical dimensions.

==== Addition

```c
double** add(double** A, double** B, int r, int c, int threads) {
    double** R = allocMatrix(r, c);
    if (!R) return NULL;

    int actualThreads = capThreads(threads, r);

    #pragma omp parallel for num_threads(actualThreads)
    for (int i = 0; i < r; i++)
        for (int j = 0; j < c; j++)
            R[i][j] = A[i][j] + B[i][j];

    return R;
}
```

The function starts by allocating the result matrix with the same dimensions as the inputs. The null check allows graceful handling of allocation failures by returning `NULL` to the caller, who can then decide how to proceed.

Thread capping occurs before the parallel region, adjusting the requested thread count to match the number of rows. The `#pragma omp parallel for` directive tells the compiler to parallelize the outer loop over rows, with OpenMP automatically dividing work among threads.

The actual computation `R[i][j] = A[i][j] + B[i][j]` is straightforward: each result element is the sum of the corresponding input elements. No synchronization needed because each thread writes to distinct memory locations in the result matrix. Different threads process different rows, so their writes never conflict.

==== Subtraction, Element-wise Multiplication

Subtraction and element-wise multiplication follow the identical structure to addition, differing only in the operation performed:

```c
// Subtraction
R[i][j] = A[i][j] - B[i][j];

// Element-wise multiplication
R[i][j] = A[i][j] * B[i][j];
```

All three operations share the same parallelization strategy and error handling, differing only in their actual operation.

==== Element-wise Division

Division includes special handling for division by zero:

```c
double** elemDiv(double** A, double** B, int r, int c, int threads) {
    double** R = allocMatrix(r, c);
    if (!R) return NULL;

    int actualThreads = capThreads(threads, r);

    #pragma omp parallel for num_threads(actualThreads)
    for (int i = 0; i < r; i++) {
        for (int j = 0; j < c; j++) {
            if (B[i][j] == 0)
                R[i][j] = 0.0 / 0.0;  // NaN
            else
                R[i][j] = A[i][j] / B[i][j];
        }
    }

    return R;
}
```

When the divisor is zero, the result is set to NaN (not a number) using the expression `0.0 / 0.0`. Since C doesn't have `NaN` as a built-in literal, this produces an IEEE 754 NaN value, which propagates through subsequent calculations and is recognizable in the output as "nan" or "-nan".

The alternative approaches of skipping zero divisors or aborting processing would either leave result positions uninitialized or prevent computing other valid divisions. Storing NaN allows the operation to complete while clearly marking problematic positions.

=== Transpose

Transposition swaps rows and columns, transforming an `r * c` matrix into a `c * r` matrix:

```c
double** transpose(double** A, int r, int c, int threads) {
    double** T = allocMatrix(c, r);
    if (!T) return NULL;

    int actualThreads = capThreads(threads, r);

    #pragma omp parallel for num_threads(actualThreads)
    for (int i = 0; i < r; i++)
        for (int j = 0; j < c; j++)
            T[j][i] = A[i][j];

    return T;
}
```

The transpose operation is always valid regardless of matrix dimensions. 

=== Matrix Multiplication

Matrix multiplication is the most computationally intensive operation, requiring $O(n^3)$ time for `n * n` matrices compared to $O(n^2)$ for element-wise operations:

```c
double** matMul(double** A, double** B, int rA, int cA, int cB, int threads) {
    double** R = allocMatrix(rA, cB);
    if (!R) return NULL;

    int actualThreads = capThreads(threads, rA);

    #pragma omp parallel for num_threads(actualThreads)
    for (int i = 0; i < rA; i++)
        for (int j = 0; j < cB; j++) {
            double sum = 0;
            for (int k = 0; k < cA; k++)
                sum += A[i][k] * B[k][j];
            R[i][j] = sum;
        }

    return R;
}
```

Matrix multiplication requires A's column count to equal B's row count. When multiplying an `rA * cA` matrix by a `cA * cB` matrix, the result is `rA * cB`. Each element of the result is computed as a dot product: `R[i][j]` = $sum$`(A[i][k] * B[k][j])`.

= Compilation and Build System

The program uses a Makefile to manage compilation:

== Build Commands

- To build the program:
```bash
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t2/src] make
gcc -Wall -Wextra -O2 -fopenmp *.c -o matrix_program
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t2/src]
```

- To clean build artifacts:

```bash
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t2/src] make clean
rm -f matrix_program results.txt
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t2/src] ls
```

This removes, the executable, and the results file, allowing a fresh build from scratch.

= Usage and Input Format

== Running the Program

The program requires two command-line arguments:

```bash
./matrix_program input.txt 4
```

Where:
- `input.txt` is the file containing matrix pairs
- `4` is the number of threads to use

= Performance Analysis

== Profiling Results

The program was tested using the `MatData.txt` dataset that was provided, running on an Arch Linux system with x86-64 architecture. The profiling results show where the program spends its time and how effectively it utilizes multiple cores.


=== Execution Time
```bash
[wizard@archlinux ~/Projects/School/HPC/Assessment/t2/code] -> time ./matrix_program ../assets/MatData.txt 4
Processing complete. Results written to results.txt
Processed 25 matrix pair(s)
./matrix_program ../assets/MatData.txt 4  0.01s user 0.00s system 259% cpu 0.003 total
```

The execution completes in just 0.003 seconds of real time, with 0.01 seconds spent in user mode and essentially zero in kernel mode. The CPU utilization of 259% shows strong multi-core usage, with the program utilizing roughly 2.6 cores actively working. This is a significant improvement over the word counter program's 128% utilization.

With 4 threads, the theoretical maximum CPU utilization would be 400% if all four threads were continuously busy. The actual utilization of 259% is reasonable given that:

- File I/O operations (reading input, writing output) are sequential
- Some matrix operations may complete too quickly to fully utilize all threads
- Thread synchronization at OpenMP barriers introduces some overhead
- The dataset size may not provide enough work to keep all threads busy continuously

The higher utilization compared to the word counter (259% vs 128%) suggests OpenMP's work distribution is more efficient than the manual pthread-based approach; granted it's not an apples to apples comparison, as one is counting words and another is operating on matrices.

=== CPU Profile (`perf report`)

Top functions by CPU time:
```bash
# Overhead  Command         Shared Object         Symbol
# ........  ..............  ....................  ..............................
    84.73%  matrix_program  libgomp.so.1.0.0      [.] gomp_barrier_wait_end
     2.52%  matrix_program  libgomp.so.1.0.0      [.] gomp_team_barrier_wait_end
     1.46%  matrix_program  libgomp.so.1.0.0      [.] gomp_init_task
     1.44%  matrix_program  libc.so.6             [.] 0x000000000005b010
     1.44%  matrix_program  libc.so.6             [.] 0x000000000005c036
     1.37%  matrix_program  libc.so.6             [.] 0x00000000000540df
     1.17%  matrix_program  libgomp.so.1.0.0      [.] gomp_team_start
     1.10%  matrix_program  libc.so.6             [.] 0x000000000005b690
     0.99%  matrix_program  libc.so.6             [.] 0x0000000000066d74
     0.74%  matrix_program  libc.so.6             [.] 0x000000000005bc47
     0.73%  matrix_program  libc.so.6             [.] 0x000000000006e033
     0.71%  matrix_program  libc.so.6             [.] _IO_file_xsputn
     0.59%  matrix_program  libc.so.6             [.] 0x000000000005b6d0
     0.48%  matrix_program  libc.so.6             [.] 0x0000000000111783
     0.16%  matrix_program  libgomp.so.1.0.0      [.] gomp_thread_start
     0.14%  matrix_program  matrix_program        [.] transpose._omp_fn.0
     0.14%  matrix_program  ld-linux-x86-64.so.2  [.] 0x0000000000008e85
     0.02%  matrix_program  ld-linux-x86-64.so.2  [.] 0x00000000000147a1
     0.01%  matrix_program  [unknown]             [k] 0xffffffffb4600087
     0.01%  matrix_program  libc.so.6             [.] 0x0000000000065838
     0.00%  matrix_program  libgomp.so.1.0.0      [.] gomp_barrier_wait
     0.00%  matrix_program  libc.so.6             [.] 0x0000000000096786
     0.00%  matrix_program  [unknown]             [k] 0xffffffffb460008f
```


==== Expected Results

- *Barrier Synchronization Dominates*: The `gomp_barrier_wait_end` function consuming 84.73% of total CPU time aligns with OpenMP's behavior for small workloads. This represents the implicit barriers at the end of each parallel region. The high percentage indicates synchronization overhead dominates actual computation for this dataset size.

- *Team Management Overhead is Present*: The combination of `gomp_team_barrier_wait_end (2.52%)`, `gomp_init_task (1.46%)`, and `gomp_team_start (1.17%)` totaling roughly 5.15% represents OpenMP's thread team management. This is unavoidable overhead as the program creates and destroys parallel regions for each operation on each matrix pair. The percentage is reasonable given the number of parallel regions created.

- *File I/O is Minimal*: The `_IO_file_xsputn` consuming only 0.71% indicates output writing isn't a bottleneck. This low overhead makes sense because the program writes relatively small amounts of formatted text, and the sequential I/O operations complete quickly compared to the parallelization overhead.

==== Unexpected Findings

- *Computation Time is Nearly Invisible*: The `transpose._omp_fn.0` function appears at only 0.14% of total time. This is surprisingly low and indicates actual matrix operations complete so quickly they barely register in the profile. 

- *High CPU Usage*: The 259% CPU utilization seems contradictory with 84.73% barrier time, but this happens because OpenMP uses active waiting (busy-spinning) rather than passive waiting. Threads consume CPU cycles spinning on barriers waiting for other threads to complete, which counts as CPU usage but accomplishes no useful work. This explains high CPU utilization without corresponding computational progress.

- *Synchronization Overhead Dominates Computation*: With approximately 85% of time spent in barriers and only 0.14% in actual computation, the synchronization overhead is roughly 600x the computation time. This is a clear indication that for this particular dataset, the parallization overhead is not ideal.


= Screenshots

#let screenshots = (
  "Screenshot from 2026-01-10 07-57-25.png",
  "Screenshot from 2026-01-10 07-57-27.png",
  "Screenshot from 2026-01-10 07-57-28.png",
  "Screenshot from 2026-01-10 07-57-29.png",
  "Screenshot from 2026-01-10 07-57-30.png",
  "Screenshot from 2026-01-10 07-57-31.png",
  "Screenshot from 2026-01-10 07-57-32.png",
  "Screenshot from 2026-01-10 07-57-36.png",
  "Screenshot from 2026-01-10 07-57-39.png",
  "Screenshot from 2026-01-10 07-57-40.png",
  "Screenshot from 2026-01-10 07-57-45.png",
  "Screenshot from 2026-01-10 07-57-46.png",
  "Screenshot from 2026-01-10 07-57-47.png",
  "Screenshot from 2026-01-10 07-57-48.png",
  "Screenshot from 2026-01-10 07-57-49.png",
  "Screenshot from 2026-01-10 07-57-53.png",
  "Screenshot from 2025-12-19 09-01-08.png",
  "Screenshot from 2025-12-19 09-02-01.png",
  "Screenshot from 2025-12-19 09-02-08.png",
  "Screenshot from 2025-12-19 09-02-10.png",
  "Screenshot from 2025-12-19 09-02-12.png",
  "Screenshot from 2025-12-19 09-02-14.png",
  "Screenshot from 2025-12-19 09-02-19.png",
  "Screenshot from 2025-12-19 09-02-22.png",
  "Screenshot from 2025-12-19 09-02-32.png",
  "Screenshot from 2025-12-19 09-02-35.png",
  "Screenshot from 2025-12-19 09-02-39.png",
  "Screenshot from 2025-12-19 09-02-43.png",

)

#for file in screenshots {
  image("images/screenshots/" + file)
}
