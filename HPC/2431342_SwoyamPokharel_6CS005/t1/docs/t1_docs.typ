#import "layout.typ": *

#show: academic-theme.with(
  title: "Implementation Of A Multithreaded Word Counter",
  author: "Swoyam Pokharel",
)

#pagebreak()

==== SUMMARY

```bash
.
├── assets
│   ├── toVerify.txt
│   └── WordOccurrenceDataset.txt
├── docs
│   ├── images
│   │   ├── herald.png
│   │   ├── screenshots
│   │   │   ├── Screenshot from 2026-01-10 07-44-19.png
│   │   │   ├── Screenshot from 2026-01-10 07-44-22.png
│   │   │   ├── Screenshot from 2026-01-10 07-44-57.png
│   │   │   ├── Screenshot from 2026-01-10 07-45-03.png
│   │   │   ├── Screenshot from 2026-01-10 07-45-05.png
│   │   │   ├── Screenshot from 2026-01-10 07-45-08.png
│   │   │   ├── Screenshot from 2026-01-10 07-45-12.png
│   │   │   ├── Screenshot from 2026-01-10 07-46-28.png
│   │   │   ├── Screenshot from 2026-01-10 07-46-30.png
│   │   │   ├── Screenshot from 2026-01-10 07-46-34.png
│   │   │   └── Screenshot from 2026-01-10 07-46-37.png
│   │   ├── Triedatastructure1.webp
│   │   └── wolverhampton.png
│   ├── layout.typ
│   ├── README.md
│   └── t1_docs.typ
├── outputs
│   ├── result.txt
│   └── WordOccurrenceDataset.txt
├── README.pdf
└── src
    ├── file_utils.c
    ├── file_utils.h
    ├── main.c
    ├── Makefile
    ├── trie.c
    ├── trie.h
    └── wordCounter

7 directories, 28 files
```
- *`README.pdf`* contains the explaination.
- *`docs/`* contains the #link("https://typst.app/")[typst] source that was used to generate this `README.pdf` along with screenshots and images used in this pdf.
- *`outputs/`* contains the `result.txt` generated by the program for the input `WordOccurrenceDataset.txt`
- *`assets/`* contains the provided input `WordOccurrenceDataset.txt`, and a `toVerify.txt` that holds the correct count of each of the words which was used to verify the program's result
- *`src/`* contains the actual source code

#pagebreak()
= Data Structures
The whole program sits on two main structures: the trie tree for managing words and their counts, and the thread data structure for coordinating work across multiple threads.

== Trie Tree
The trie (or prefix tree) is where all the actual storage happens. I could've gone with hash tables or arrays, but tries give you alphabetical ordering for free and handle shared prefixes efficiently. Each node represents a character, paths from the root spell out words, and a flag marks where words actually end.

\
\

#figure(
  image("./images/Triedatastructure1.webp", width: 80%),
  caption: [Trie Tree Example],
)

#pagebreak()

=== Structure & Components
Each node in the trie represents a single character and packs three things:

```c
typedef struct TrieNode {
  struct TrieNode* children[ALPHABET_SIZE];  // 26 pointers (a-z)
  int count;                                  // Word frequency
  pthread_mutex_t lock;                       // Thread safety
} TrieNode;
```

The `children` array holds 26 pointers, one per lowercase letter. Looking up the next character is just `character - 'a'` to get the index - 'a' maps to 0, 'b' to 1, 'z' to 25. No hashing, no comparisons, just direct array access. This makes traversal fast.

The `count` field tracks how many times a word ending at this node showed up. Zero means this node is just part of a path, not an actual word ending. Take "cat" and "catch" - the 't' in "cat" gets a non-zero count marking the word end, while the intermediate nodes in "catch" stay at zero until you hit the final 'h'.

#scale(120%)[
  #oxdraw(
    "
    graph LR
    root((ROOT))
    root --> C[C]
    C --> A[A]
    A --> T[T<br/>count=1]

    T --> c2[C<br/>count=0]
    c2 --> H[H<br/>count=1]

    T -.-> note1[cat ends here]
    H -.-> note3[catch ends here]
    ",
  )
]

The `lock` mutex is what makes this thread-safe. Instead of one massive lock for the whole trie (which would kill any parallelism), each node gets its own lock. This means multiple threads can work on different parts of the trie simultaneously without stepping on each other's toes.

=== Why Trie (automatic sorting, memory efficiency)
I picked tries over hash tables or arrays for a few reasons:

- *Automatic Alphabetical Sorting*: Since we need sorted output anyway, tries are perfect. The structure itself maintains alphabetical order - just traverse children from index 0-25 and you visit nodes alphabetically. No extra sorting step needed, which would be expensive with hash tables. The sorted output basically comes for free.

- *Thread-Safe Expansion:* The trie handles concurrent insertions naturally through fine-grained locking. Each path can be locked independently, so multiple threads inserting different words don't interfere with each other. This makes tries particularly well-suited for multithreaded word counting - threads rarely fight for the same nodes unless they're processing very similar words.

- *Memory Efficiency for Shared Prefixes:* Words sharing common prefixes share the same nodes in memory. With hash tables, each word needs separate storage even for shared prefixes. For large dictionaries with many similar words, tries save significant memory. Given our input dataset is pretty big, this efficiency matters:

```bash
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/assets] -> wc -w WordOccurrenceDataset.txt
120000 WordOccurrenceDataset.txt
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/assets] ->
```

With 120,000 words, the memory savings from shared prefixes become meaningful, particularly for common prefixes that appear across thousands of words.

=== Operations (Insert, Write)

==== Word Insertion:

The insertion process walks through the trie character by character, creating nodes as needed. Consider inserting "apple" into an empty trie:

1. Start at root, look for child 'a' (index 0)
2. Move to 'a' node, look for child 'p' (index 15)
3. Move to 'p' node, look for child 'p' (index 15)
4. Move to second 'p' node, look for child 'l' (index 11)
5. Move to 'l' node, look for child 'e' (index 4)
6. At final 'e' node, increment count

If any child doesn't exist during traversal, it needs to be created immediately. This is also where multiple threads might try to create the same child node simultaneously, causing a race condition.

```c
void insert_word_into_trie(TrieNode* root, const char* word) {
  TrieNode* current = root;

  for (int i = 0; word[i] != '\0'; i++) {
    char c = tolower(word[i]);

    // Skip non-alphabetic ( if any )
    if (c < 'a' || c > 'z') continue;

    int index = c - 'a';

    if (current->children[index] == `NULL`) {
      pthread_mutex_lock(&current->lock);
      if (current->children[index] == `NULL`) {
        current->children[index] = create_trie_node();
      }
      pthread_mutex_unlock(&current->lock);
    }

    current = current->children[index];
  }

  pthread_mutex_lock(&current->lock);
  current->count++;
  pthread_mutex_unlock(&current->lock);
}
```

==== The Double-Check Locking Pattern
Notice the code checks if `children[index] == NULL` twice. Looks redundant at first, but it's actually an optimization pattern called double-check locking.

The first check, done without grabbing the lock, lets threads quickly skip the locking mechanism when a child already exists. Acquiring and releasing locks is expensive, so avoiding them when possible gives significant performance gains.

But if the first check finds the child is NULL, we need to create it. Here's the problem: between the first check and acquiring the lock, another thread might've already created that same child node. Classic race condition.

Consider two threads inserting "apple" and "apply" simultaneously:
- Thread 1 checks: `children[p]` is NULL
- Thread 2 checks: `children[p]` is NULL
- Thread 1 grabs lock, creates the `p` node, releases lock
- Thread 2 grabs lock, but without the second check, it'd create another `p` node, overwriting Thread 1's node and losing all the data stored in it

The second check, done inside the lock after acquisition, prevents this race condition. After grabbing the lock, we verify that no other thread created the child while we were waiting. If another thread beat us to it, we just skip creation and move on. If the child is still NULL, we're guaranteed to be the only thread that can create it right now, ensuring no data loss or corruption


==== Output Writing
The output writing does a simple depth-first traversal of the trie, building words character by character and writing them when it hits complete words:

```c
void write_trie_to_file(TrieNode* node, char* prefix, int depth, FILE* fp) {
  if (node->count > 0) {
    prefix[depth] = '\0';
    fprintf(fp, "%s: %d\n", prefix, node->count);
  }

  for (int i = 0; i < ALPHABET_SIZE; i++) {
    if (node->children[i] != `NULL`) {
      prefix[depth] = 'a' + i;
      write_trie_to_file(node->children[i], prefix, depth + 1, fp);
    }
  }
}
```

The function maintains a prefix string representing the current path from the root. At each node with a non-zero count, it writes the complete word formed by the prefix along with its frequency. Since the children array is iterated from index 0 to 25 (corresponding to 'a' through 'z'), the output is automatically alphabetically sorted. No post-processing sort step needed, which would be expensive for large datasets.

While recursion can sometimes cause stack overflow issues with deeply nested structures, the depth of a trie is bcwjust the length of the longest word, which we can reasonably assume stays typically under 30 characters for most cases. This makes the recursive approach safe here.

=== Memory Management

==== Node Allocation

Each trie node is allocated using `calloc` rather than `malloc`,

```c
TrieNode* create_trie_node() {
    TrieNode* node = calloc(1, sizeof(TrieNode));
    if (!node) {
        fprintf(stderr, "Error: Memory allocation failed\n");
        exit(1);
    }
    pthread_mutex_init(&node->lock, `NULL`);
    return node;
}
```

Using `calloc` instead of `malloc` initializes all memory to zero, automatically setting all 26 child pointers to NULL and the count to 0. This initialization is crucial because the insertion logic explicitly depends on `NULL` pointers to identify missing children. Using `malloc` would leave memory uninitialized with whatever garbage values happened to be there, which would completely break the insertion algorithm.

The mutex is initialized immediately after allocation to ensure it's ready before any thread can possibly access the node.

==== Recursive Destruction:

The trie is destroyed using post-order traversal; children get freed before their parent node:

```c
void destroy_trie(TrieNode* node) {
    if (node == `NULL`) return;

    for (int i = 0; i < ALPHABET_SIZE; i++) {
        if (node->children[i] != `NULL`) {
            destroy_trie(node->children[i]);
        }
    }

    pthread_mutex_destroy(&node->lock);
    free(node);
}
```

This traversal order ensures no node is freed while still referenced by a parent's children array. The mutex is destroyed before freeing the node to properly release system resources associated with the lock. If we freed nodes in pre-order (parent before children), we'd lose the pointers to children, causing a memory leak as those child nodes would become unreachable but remain allocated.

== Thread Data Structure

Each worker thread gets its own `ThreadData` structure containing all the info needed to process its assigned chunk of work independently:

```c
typedef struct {
    TrieNode* root;        // Shared trie root
    char** words;          // All words from file
    int total_words;       // Total word count
    int thread_id;         // This thread's ID
    int num_threads;       // Total number of threads
} ThreadData;
```

- The `root` pointer gives each thread access to the shared trie where all words get inserted. While the trie itself is shared among all threads, each thread maintains its own copy of this pointer in its ThreadData structure.

- The `words` array contains all words read from the input file. This array is shared among all threads in a read-only fashion. No thread modifies the array itself, they only read words from it to insert into the trie. This eliminates the need for synchronization when accessing the words array, since reads are inherently thread-safe.

- The `total_words` field specifies the length of the words array, letting each thread know when to stop processing without needing to search for a sentinel value or null terminator.

- The `num_threads` field tells each thread the total number of threads in the pool, necessary for calculating the distribution of work for the round robin algorithm. Together with `thread_id`, this lets each thread independently determine its workload without any coordination.

=== Memory Management
The ThreadData structures are allocated on the heap in a single contiguous block:

```c
ThreadData* thread_data = malloc(num_threads * sizeof(ThreadData));
```

This allocates an array of `ThreadData` structures on the heap. Each structure is then initialized with pointers to shared resources (root and words) and unique values (thread_id and num_threads).

ThreadData structures don't own the memory they point to. The root and words pointers reference memory allocated elsewhere and managed separately. The main thread owns the trie and words array, while the `ThreadData` array is just temporary for passing info to worker threads. This means when we free the thread_data array, we only free the array of structures themselves, not the trie or words array:


```c
free(thread_data);  // Only frees the ThreadData array
// root and words are freed separately
```

The main thread is responsible for allocating the trie and words array before creating worker threads, and for freeing them after all threads have completed.

= Program Flow & File Parsing

== Execution Pipeline

The program runs through seven sequential phases:

1. Parse command-line arguments (thread count, filename)
2. Read all words into memory
3. Create shared trie root
4. Allocate and initialize ThreadData structures
5. Launch worker threads
6. Wait for completion (`pthread_join`)
7. Write results and cleanup

```c
int main(int argc, char* argv[]) {
    int num_threads = atoi(argv[1]);
    char* input_filename = argv[2];

    int total_words;
    char** words = read_words_from_file(input_filename, &total_words);

    TrieNode* root = create_trie_node();

    pthread_t* threads = malloc(num_threads * sizeof(pthread_t));
    ThreadData* thread_data = malloc(num_threads * sizeof(ThreadData));

    for (int i = 0; i < num_threads; i++) {
        pthread_create(&threads[i], `NULL`, count_words_in_thread,
                      &thread_data[i]);
    }

    for (int i = 0; i < num_threads; i++) {
        pthread_join(threads[i], `NULL`);
    }

    write_trie_to_file(root, prefix, 0, output);

    destroy_trie(root);
    free(words);
    free(threads);
    free(thread_data);
}
```

The main thread does setup, spawns worker threads to do the computational work, waits for all workers to finish, then cleans up.

The separation between reading words and processing them was intentional. By reading all words upfront, we avoid file I/O during the parallel processing phase. As file I/O is inherently sequential and would create a bottleneck if threads tried reading from the file simultaneously.

== Reading Words
The file reading process handles the input line by line, building a dynamically growing array of strings:

```c
char** read_words_from_file(const char* filename, int* word_count) {
    FILE* fp = fopen(filename, "r");

    int capacity = 1000;
    int count = 0;
    char** words = malloc(capacity * sizeof(char*));
    char buffer[MAX_WORD_LENGTH];

    while (fgets(buffer, sizeof(buffer), fp)) {
        buffer[strcspn(buffer, "\n\r")] = '\0';
        if (strlen(buffer) == 0) continue;

        if (count >= capacity) {
            capacity *= 2;
            words = realloc(words, capacity * sizeof(char*));
        }

        words[count] = strdup(buffer);
        count++;
    }

    *word_count = count;
    return words;
}
```

The function reads line by line using `fgets`, strips newlines with `strcspn` (which finds the first occurrence of any newline character and replaces it with a null terminator), skips empty lines to avoid inserting empty strings into the trie, and duplicates each word using `strdup`, which allocates new memory for the string.

== Memory Management

=== Dynamic Array Growth

The array starts with capacity 1000. When this capacity is reached, the array doubles in size using `realloc`. This doubling strategy provides amortized `O(1)` insertion time; i.e over many insertions, the average cost per insertion remains constant despite the occasional expensive resize operation.

For our dataset containing 120,000 words, this results in roughly 7-8 reallocations ($log_2$(120000/1000)). Each reallocation copies the existing array to a new, larger location, but since these happen exponentially less frequently as the array grows, the total copying overhead stays reasonable. Although, in our case we already know the size of the dataset, and we could've simply allocated the exact space required as to bypass this reallocation overhead, this was put in for generalized file reading.

```c
if (count >= capacity) {
    capacity *= 2;
    words = realloc(words, capacity * sizeof(char*));
}
```
=== Cleanup

Memory must be freed in reverse order of allocation, ensuring we never try to free memory through a pointer that's already been freed:

```c
for (int i = 0; i < total_words; i++) {
    free(words[i]);  // Free individual strings first
}
free(words);  // Then free array
```

We must free the individual strings before freeing the array because each string was allocated separately by `strdup`. The words array itself only contains pointers to these strings, not the string data. If we freed the array first, we'd lose access to the string pointers, making it impossible to free the string data and causing a memory leak.

= Multithreading Implementation

== Work Distribution

The round-robin distribution assigns work to threads in a cyclic pattern: thread `i` processes indices `i, i+n, i+2n, i+3n, ...` where `n = num_threads`. This creates natural load balancing without requiring any coordination between threads.

```c
void* count_words_in_thread(void* arg) {
    ThreadData* data = (ThreadData*)arg;

    for (int i = data->thread_id; i < data->total_words; i += data->num_threads) {
        insert_word_into_trie(data->root, data->words[i]);
    }

    return `NULL`;
}
```

Each thread starts at its own unique offset (thread_id) and increments by the total number of threads (num_threads) on each iteration. This ensures no two threads ever try to process the same word, eliminating the need for any synchronization when accessing the words array.

=== Load Balancing

The maximum difference between any two threads is exactly one word. This property holds regardless of the total word count or number of threads, making the distribution scheme very flexible.

Example with 13 words and 4 threads:

```
Thread 0: 0, 4, 8, 12     (4 words)
Thread 1: 1, 5, 9         (3 words)
Thread 2: 2, 6, 10        (3 words)
Thread 3: 3, 7, 11        (3 words)
```

The first thread gets one extra word because 13 doesn't divide evenly by 4, but this imbalance is minimal.

== Thread Synchronization

=== Race Conditions

Without proper synchronization, two primary race conditions could corrupt the trie structure and produce incorrect results:

*Node Creation Race:* Two threads detect a missing child node and both try to create it. Without synchronization, both would allocate and initialize a new node, and one would overwrite the other's pointer in the children array. This overwrites the first node created, causing a memory leak (the overwritten node never gets freed) and data loss (any words already inserted into that subtree are lost).

*Count Increment Race:* The operation `count++` looks like one single operation but actually consists of three separate machine instructions: read the current value, increment it, write the new value back. If two threads execute this sequence simultaneously, they might both read the same initial value, both increment it to the same new value, and both write that same value back.

=== Solution: Fine-Grained Locking

The solution uses fine-grained locking; each node gets its own mutex instead of using a single global lock for the entire trie:

```c
typedef struct TrieNode {
    struct TrieNode* children[ALPHABET_SIZE];
    int count;
    pthread_mutex_t lock;  // One lock per node
} TrieNode;
```

This design lets multiple threads work on different paths through the trie simultaneously without interfering. Locks are grabbed only in two specific cases:

1. Creating a child node (lock the parent)
2. Incrementing the count (lock the final node)

Imagine two threads inserting completely different words, like "apple" and "zebra". These threads follow completely different paths through the trie, touching different nodes. With fine-grained locking, they can proceed completely independently, never waiting for each other. The alternative would be a single global lock for the entire trie, which would serialize all insertions. Threads would have to wait for each other even when working on completely unrelated parts of the trie, effectively killing any benefit from parallelization.

=== Memory Barriers

Mutex operations provide implicit memory barriers that ensure proper synchronization of memory accesses across threads. When thread A releases a lock and thread B subsequently grabs the same lock, B is guaranteed to see all memory modifications made by A before the release. For the trie, this means:

- When a thread creates a child node and releases the lock, any other thread that later grabs that lock will see the newly created child
- When a thread increments a count and releases the lock, any other thread that later grabs that lock will see the updated count

Without these guarantees (provided automatically by the mutex implementation), the program would have produced incorrect results.

= Compilation Instructions

===== To Run The Code With A Default Of 4 Threads And Provided Input Dataset:
```bash
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/code] -> ls
file_utils.c  file_utils.h  main.c  Makefile  trie.c  trie.h
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/code] -> make run
```

===== To Run The Code With A Custom Number Of Threads And Custom Dataset:

```c
// alternatively to compile:
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/code] -> make

// compile:
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/code] -> gcc *.c -Wall -Wextra -O2

// run
[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/code] -> ./wordCounter -h
Usage: ./wordCounter <num_threads> <input_file>
Example: ./wordCounter 4 input.txt

[wizard@archlinux ~/Projects/School/HPC/Assessment/t1/code] ->
```

= Performance Analysis

== Profiling Results

The program was tested using a dataset containing 120,000 words from `WordOccurrenceDataset.txt`, running on an Arch Linux system with x86-64 architecture. The profiling results provide insight into where the program spends its time and how effectively it utilizes multiple cores.

=== Execution Time

Running the program with 4 threads gives these results using the `time` command:

```bash
$ time ./word_counter 4 WordOccurrenceDataset.txt
0.01s user 0.00s system 128% cpu 0.012 total
```

The execution completes in just 0.012 seconds of real time, with 0.01 seconds spent in user mode and essentially zero in kernel mode. The CPU utilization of 128% indicates the program is using more than one core, which is what we expect from a multithreaded application. This value represents the sum of CPU time across all threads divided by real time, so 128% means the program is utilizing the equivalent of about 1.3 cores actively working.

With 4 threads, the theoretical maximum CPU utilization would be 400% if all four threads were continuously busy. The lower actual utilization of 128% occurs because the program includes sequential phases, particularly file I/O and the final output writing, where only one thread is active. Additionally, synchronization overhead from acquiring and releasing locks, and time spent in memory allocation, contributes to threads occasionally waiting rather than computing.

=== CPU Profile (`perf report`)

Top functions by CPU time:

```
  16.32%  wordCounter    wordCounter           [.] insert_word_into_trie
  13.08%  wordCounter    libc.so.6             [.] 0x00000000000a4f15
  11.37%  wordCounter    libc.so.6             [.] 0x000000000009b9fc
   7.03%  wordCounter    wordCounter                 [.] read_words_from_file
   5.82%  wordCounter    libc.so.6             [.] malloc
   5.27%  wordCounter    libc.so.6             [.] _IO_fgets
   4.58%  wordCounter    libc.so.6             [.] pthread_mutex_lock
   4.13%  wordCounter    libc.so.6             [.] 0x000000000018d60c
   3.74%  wordCounter    libc.so.6             [.] _IO_getline_info
   3.29%  wordCounter    libc.so.6             [.] 0x000000000017d4ef
   2.61%  wordCounter    libc.so.6             [.] 0x000000000018d6d8
   2.47%  wordCounter    libc.so.6             [.] cfree
   2.20%  wordCounter    libc.so.6             [.] 0x00000000000a6431
   2.18%  wordCounter    libc.so.6             [.] 0x000000000017d4c4
   2.17%  wordCounter    wordCounter                 [.] main
   2.09%  wordCounter    [unknown]             [k] 0xffffffff89401280
   2.00%  wordCounter    libc.so.6             [.] 0x00000000000a686c
   1.91%  wordCounter    libc.so.6             [.] 0x0000000000180da8
   1.21%  wordCounter    libc.so.6             [.] 0x0000000000186de8
   1.09%  wordCounter    libc.so.6             [.] 0x0000000000180da2
   1.00%  wordCounter    libc.so.6             [.] 0x000000000009b9f0
   0.93%  wordCounter    libc.so.6             [.] 0x00000000000a643d
   0.83%  wordCounter    wordCounter                 [.] count_words_in_thread
   0.83%  wordCounter    libc.so.6             [.] __strdup
   0.81%  wordCounter    libc.so.6             [.] 0x0000000000186e37
   0.69%  wordCounter    libc.so.6             [.] 0x000000000018d68f
   0.20%  wordCounter    ld-linux-x86-64.so.2  [.] 0x0000000000014dbd
   0.04%  wordCounter    libc.so.6             [.] 0x00000000000a418d
   0.03%  wordCounter    libc.so.6             [.] 0x00000000000a6441
   0.03%  wordCounter    libc.so.6             [.] 0x00000000000a6e82
   0.03%  wordCounter    ld-linux-x86-64.so.2  [.] 0x0000000000014787
   0.02%  wordCounter    libc.so.6             [.] 0x00000000000a6aed
   0.00%  wordCounter    libc.so.6             [.] 0x0000000000065404
   0.00%  wordCounter    ld-linux-x86-64.so.2  [.] 0x000000000001f70a
   0.00%  wordCounter    libc.so.6             [.] 0x0000000000186dd3
   0.00%  wordCounter    libc.so.6             [.] 0x000000000005b56d
   0.00%  wordCounter    libc.so.6             [.] 0x0000000000186d84
   0.00%  wordCounter    libc.so.6             [.] 0x00000000001803b0
   0.00%  wordCounter    libc.so.6             [.] __ctype_init
   0.00%  wordCounter    libc.so.6             [.] 0x0000000000180380
   0.00%  wordCounter    libc.so.6             [.] 0x000000000009676c
   0.00%  wordCounter    libc.so.6             [.] 0x00000000000966e0
   0.00%  wordCounter    libc.so.6             [.] 0x000000000011aa00
   0.00%  wordCounter    ld-linux-x86-64.so.2  [.] 0x000000000001eb43
   0.00%  wordCounter    ld-linux-x86-64.so.2  [.] 0x000000000001f6ab
   0.00%  wordCounter    libc.so.6             [.] 0x000000000011aa07
```

==== Expected Results

- *Insertion Dominates Execution:* The `insert_word_into_trie` function consuming 16.32% of total CPU time aligns perfectly with expectations. This is the computational core where actual work happens. With 120,000 words being processed, this function executes 120,000 times, making it the natural hotspot.

- *File I/O is Significant:* The combination of `read_words_from_file` (7.03%) and `_IO_fgets` (5.27%) totaling roughly 12.3% represents the sequential file reading phase. This is unavoidable as every word must be read from disk into memory before processing.

- *Low Lock Contention:* The `pthread_mutex_lock` consuming only 4.58% is what was expected. This low overhead comes from the fine-grained locking strategy. If threads were frequently fighting for the same locks, we'd see this percentage much higher.

==== Unexpected Findings

- *Memory Allocation Overhead is Substantial:* The combined memory management overhead of `malloc` (5.82%), various libc memory functions (~24.45% total), and `cfree` (2.47%) consumes roughly 32.74% of total execution time. This is significantly higher than I had anticipated. With the trie potentially containing tens of thousands of nodes, the cumulative cost seems to have become substantial.

  - This suggests a potential optimization: implementing a custom memory pool for trie nodes. Instead of calling `malloc` for each node individually, we could pre-allocate large blocks and subdivide them ourselves.


= Screenshots
#let screenshots = (
  "Screenshot from 2026-01-10 15-04-11.png",
  "Screenshot from 2026-01-10 15-04-13.png",
  "Screenshot from 2026-01-10 07-44-19.png",
  "Screenshot from 2026-01-10 07-44-22.png",
  "Screenshot from 2026-01-10 07-44-57.png",
  "Screenshot from 2026-01-10 07-45-03.png",
  "Screenshot from 2026-01-10 07-45-05.png",
  "Screenshot from 2026-01-10 07-45-08.png",
  "Screenshot from 2026-01-10 07-45-12.png",
  "Screenshot from 2026-01-10 07-46-28.png",
  "Screenshot from 2026-01-10 07-46-30.png",
  "Screenshot from 2026-01-10 07-46-34.png",
  "Screenshot from 2026-01-10 07-46-37.png",
)

#for file in screenshots {
  image("images/screenshots/" + file)
}
