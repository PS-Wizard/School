#import "layout.typ": *

#show: academic-theme.with(
  title: "Implementation Of A GPU Powered Edge Detection Program",
  author: "Swoyam Pokharel",
)

#pagebreak()

==== SUMMARY

```bash
.
├── assets
│   ├── lodepng.cpp
│   ├── lodepng.h
│   └── lodepng.o
├── docs
│   ├── images
│   │   ├── herald.png
│   │   ├── screenshots
│   │   │   ├── cat.png
│   │   │   ├── edge_cat.png
│   │   │   ├── input.png
│   │   │   ├── output.png
│   │   │   ├── Screenshot from 2026-01-10 14-28-50.png
│   │   │   ├── Screenshot from 2026-01-10 14-28-52.png
│   │   │   ├── Screenshot from 2026-01-10 14-28-53.png
│   │   │   └── Screenshot from 2026-01-10 14-28-54.png
│   │   └── wolverhampton.png
│   ├── layout.typ
│   ├── t4_docs.pdf
│   └── t4_docs.typ
├── output
│   ├── cat.png
│   ├── edge_cat.png
│   ├── input.png
│   └── output.png
└── src
    ├── edge_detection.cu
    ├── edge_detector
    ├── edge_img.png
    └── Makefile

7 directories, 27 files
```

- *`README.md`* contains the explanation
- *`docs/`* contains the #link("https://typst.app/")[typst] source that was used to generate this `README.pdf` along with screenshots and images used in this pdf.
- *`output/`* contains two pairs of images (input, edge detected output) generated by the program
- *`assets/`* contains the lodepng library files
- *`src/`* contains the actual program source code

#pagebreak()

= Program Architecture

The program implements Sobel edge detection accelerated using CUDA for GPU execution. The Edge detection is about finding boundaries in images by identifying regions where pixel intensity changes sharply.

== Why GPU Acceleration?

Edge detection is intuitively parallellizable. Each output pixel can be computed independently by examining its `3x3` neighborhood, which makes it perfect for GPU parallelization where thousands of threads can process different pixels simultaneously.

= The Sobel Operator

== Mathematical Foundation

The Sobel operator detects edges by measuring how quickly pixel intensity changes in both horizontal and vertical directions. It applies two convolution kernels to compute gradients:

*Horizontal Gradient (Gx):*
```
-1  0  +1
-2  0  +2
-1  0  +1
```

*Vertical Gradient (Gy):*
```
-1  -2  -1
 0   0   0
+1  +2  +1
```

== Why These Specific Kernels?

Edges are locations where brightness changes sharply. To find them, we measure how quickly brightness changes, which is essentially computing a derivative. In calculus, a derivative measures rate of change; here with images, we're measuring how pixel brightness changes as we move across the image.

=== Building the Sobel Kernel

The Sobel kernel combines two mathematical operations that work together to detect edges while dealing with noise.

==== 1. Taking a Derivative

The simplest derivative approximation subtracts neighboring pixels:

#align(center)[
  `[-1  0  +1]`
]

This is called a central difference: it subtracts the left pixel from the right pixel to measure the horizontal rate of change. The result tells us how much brightness changed in the x-direction.

==== 2. Smoothing Perpendicular to the Derivative

Real images contain noise, as such a single bright speck shouldn't register as an edge. To reduce noise sensitivity, we smooth the image in the perpendicular direction using a weighted average:

#align(center)[
  $vec(1, 2, 1)$
]

The center pixel receives weight 2, while neighbors receive weight 1. This is a binomial filter derived from Pascal's triangle that approximates Gaussian smoothing. #link("https://www.sciencedirect.com/topics/engineering/sobel-operator")[The weights [1, 2, 1] form the third row of Pascal's triangle and provide noise reduction].

==== 3. Combining via Outer Product

The Sobel kernel is constructed by taking the outer product of the smoothing filter and the derivative. The outer product of two vectors creates a matrix where each element equals the product of corresponding vector elements.

For *Gx* (detects vertical edges):

$ vec(1, 2, 1) times.o vec(-1, 0, +1) = mat(-1, 0, +1; -2, 0, +2; -1, 0, +1) $

- Row 1: $1 times vec(-1, 0, +1) = (-1, 0, +1)$
\
- Row 2: $2 times vec(-1, 0, +1) = (-2, 0, +2)$ < The 2x weighting appears here
\
- Row 3: $1 times vec(-1, 0, +1) = (-1, 0, +1)$

For *Gy* (detects horizontal edges):

$ vec(-1, 0, +1) times.o vec(1, 2, 1) = mat(-1, -2, -1; 0, 0, 0; +1, +2, +1) $

Breaking this down:
- Row 1: $-1 times vec(1, 2, 1) = (-1, -2, -1)$
\
- Row 2: $0 times vec(1, 2, 1) = (0, 0, 0)$
\
- Row 3: $+1 times vec(1, 2, 1) = (+1, +2, +1)$

The middle column has values `-2, 0, +2`. These larger values come from the `2` in the smoothing kernel `[1, 2, 1]`, which weights the center pixel more heavily than its neighbors.

== Understanding Gx and Gy

The program uses two separate kernels because edges can occur in any direction.

*Gx (Horizontal Gradient):* Detects vertical edges by measuring how brightness changes horizontally (left to right):
#align(center, [
  ```
  -1  0  +1
  -2  0  +2
  -1  0  +1
  ```
])

- Left column: negative weights
- Center column: zero (reference point)
- Right column: positive weights
- Vertical direction: smoothed with [1, 2, 1] weights

When we slide this kernel across the image, if the left side is dark and right side is bright, we get a large positive value, thus a vertical edge is detected.

*Gy (Vertical Gradient):* Detects *horizontal edges* by measuring how brightness changes vertically (top to bottom):

#align(center, [
  ```
  -1  -2  -1
   0   0   0
  +1  +2  +1
  ```
])

- Top row: negative weights
- Middle row: zero (reference point)
- Bottom row: positive weights
- Horizontal direction: smoothed with [1, 2, 1] weights

When we slide this kernel across the image, if the top is dark and bottom is bright, we get a large positive value; a horizontal edge is detected.

== What is Convolution

Convolution is a weighted sum of neighbors. Say you're at pixel position (5, 5). The kernel covers pixels from (4,4) to (6,6):

#align(center, [
  ```
  Image region:        Kernel:
  [10  20  30]         [-1  0  +1]
  [40  50  60]    *    [-2  0  +2]
  [70  80  90]         [-1  0  +1]
  ```
])

#align(center, [
  ```
  10*(-1) = -10      20*0 = 0       30*(+1) = +30
  40*(-2) = -80      50*0 = 0       60*(+2) = +120
  70*(-1) = -70      80*0 = 0       90*(+1) = +90
  ```
])

#align(center, [
  ```
  output[5,5] = -10 + 0 + 30 + (-80) + 0 + 120 + (-70) + 0 + 90 = 80
  ```
])

The process:
1. Center the kernel on a pixel
2. Multiply each neighbor by the corresponding kernel value
3. Sum all the products

The kernel values act as weights determining how much each neighbor contributes to the result. For edge detection:

- Neighbors on one side contribute negatively (if dark, they give negative values)
- Neighbors on the other side contribute positively (if bright, they give positive values)
- If there's uniform brightness, the contributions cancel to zero (no edge)
- If there's a sharp transition, we get a large magnitude (edge detected)

== Combining the Gradients

By computing both Gx and Gy and combining them with:

$ "magnitude" = sqrt("gx"^2 + "gy"^2) $

we detect edges regardless of their orientation. This formula computes the euclidean length of the vectors, giving us the overall edge strength at each pixel.

= CUDA Implementation

== Memory Allocation

=== Host Memory (CPU)

```c
unsigned char* gray = (unsigned char*)malloc(num_pixels);
unsigned char* output = (unsigned char*)malloc(num_pixels);
```

The program allocates two buffers: `gray` holds the grayscale input image, and `output` receives the edge-detected result after GPU processing.

=== Device Memory (GPU Global Memory)

```c
unsigned char *d_input, *d_output;
cudaMalloc(&d_input, bytes);
cudaMalloc(&d_output, bytes);
```

Device memory resides on the GPU itself. The `d_input` and `d_output` pointers reference GPU memory.

=== Constant Memory

```c
__constant__ int Gx[9] = { -1, 0, 1, -2, 0, 2, -1, 0, 1 };
__constant__ int Gy[9] = { -1, -2, -1, 0, 0, 0, 1, 2, 1 };
```

Constant memory is a special GPU memory space optimized for read only data that all threads access. This provides significant performance benefits such as:

- Single memory read broadcasts to all threads in a warp (group of 32 threads)
- Cached aggressively with very low latency
- Perfect for data that never changes and is read by every thread

Since every thread needs to read the exact same Gx and Gy values to compute its convolution, constant memory is ideal.

== Memory Transfer

=== Host to Device

```c
cudaMemcpy(d_input, gray, bytes, cudaMemcpyHostToDevice);
```

This copies the grayscale image from CPU memory to GPU memory. The `cudaMemcpyHostToDevice` flag specifies the transfer direction. This operation is synchronous, meaning the CPU waits for the transfer to complete before continuing.

=== Device to Host

```c
cudaMemcpy(output, d_output, bytes, cudaMemcpyDeviceToHost);
```

After GPU processing completes, this copies the edge-detected result back to CPU memory. The opposite flag `cudaMemcpyDeviceToHost` specifies transfer from GPU to CPU.

= Kernel Function

== Thread Organization

CUDA organizes threads into a two parts: threads within blocks, and blocks within a grid.

=== Thread Blocks

```c
dim3 threads(16, 16);
```

Each thread block contains `16 * 16 = 256` threads arranged in a 2D grid. This creates a mapping to image pixels: each thread processes one pixel. The `16*16` size is chosen because:

- 256 threads per block is below the maximum (typically 1024)
- Powers of 2 often optimize memory access patterns

=== Grid Dimensions

```c
dim3 blocks(CEIL_DIV(width, 16), CEIL_DIV(height, 16));
```

The grid consists of enough blocks to cover the entire image. The `CEIL_DIV` macro ensures we create enough blocks even when image dimensions aren't exact multiples of 16:

```c
#define CEIL_DIV(x, y) (((x) + (y) - 1) / (y))
```

For a `1920 x 1080` image:
- Blocks in x-direction: `CEIL_DIV(1920, 16) = 120`
- Blocks in y-direction: `CEIL_DIV(1080, 16) = 68`
- Total blocks: `120 * 68 = 8,160`
- Total threads: `8,160 * 256 = 2,088,960`

However, this creates more threads than pixels (2,088,960 threads for 2,073,600 pixels), which is why we need bounds checking in the kernel.

== Kernel Function

The `sobelKernel` function executes on the GPU with each thread processing one pixel:

```c
__global__ void sobelKernel(unsigned char* input, unsigned char* output, int width, int height)
{
    int col = threadIdx.x + blockIdx.x * blockDim.x;
    int row = threadIdx.y + blockIdx.y * blockDim.y;

    if (col >= width || row >= height)
        return;
```

The `__global__` qualifier marks this as a kernel function callable from the host but executing on the device.

The bounds check `if (col >= width || row >= height) return;` handles the case where the grid of blocks doesn't align perfectly with the image dimensions. Threads beyond the image boundary exit immediately without processing.

=== Convolution Loop

```c
int gx_sum = 0;
int gy_sum = 0;

for (int ky = -1; ky <= 1; ky++) {
    for (int kx = -1; kx <= 1; kx++) {
        int n_row = row + ky;
        int n_col = col + kx;

        unsigned char pixel_value = 0;
        if (n_row >= 0 && n_row < height && n_col >= 0 && n_col < width) {
            pixel_value = input[n_row * width + n_col];
        }

        int kernel_idx = (ky + 1) * 3 + (kx + 1);

        gx_sum += pixel_value * Gx[kernel_idx];
        gy_sum += pixel_value * Gy[kernel_idx];
    }
}
```

The nested loops iterate over the `3*3` neighborhood around the current pixel. For each neighbor:

1. Compute neighbor coordinates (`n_row`, `n_col`)
2. Check if neighbor is within image bounds
3. Read neighbor pixel value (or use 0 for out-of-bounds)
4. Compute kernel array index from relative position
5. Multiply pixel by both kernel weights and accumulate

The kernel index calculation `(ky + 1) * 3 + (kx + 1)` converts from relative coordinates (-1, 0, 1) to array indices (0-8). When `ky=-1` and `kx=-1`, this produces index 0 (top-left). When `ky=1` and `kx=1`, this produces index 8 (bottom-right).

For pixels at image boundaries, some neighbors fall outside the image. So,treating these as 0, creates a implicit zero-padding effect.

=== Magnitude Calculation

```c
float magnitude = sqrtf((float)(gx_sum * gx_sum + gy_sum * gy_sum));

if (magnitude > 255)
    magnitude = 255;

output[row * width + col] = (unsigned char)magnitude;
```

After accumulating both gradients, we compute their magnitude. The clamping operation `if (magnitude > 255) magnitude = 255;` ensures the result fits in an 8-bit unsigned integer.

The final write `output[row * width + col]` stores the result in global memory. The `row * width + col` converts 2D coordinates to a 1D array offset. Different threads write to different locations, so no synchronization is needed.

= Image Processing

== Command Line Interface
```c
int main(int argc, char* argv[])
{
    if (argc < 2) {
        printf("Usage: %s <image1.png> [image2.png] [...]\n", argv[0]);
        return 1;
    }

    printf("Processing %d image(s)...\n", argc - 1);

    for (int i = 1; i < argc; i++) {
        char* output_path = createOutputFilename(argv[i]);
        processImage(argv[i], output_path);
        free(output_path);
        printf("\n");
    }

    printf("All images processed!\n");
    return 0;
}
```

The program accepts image file paths as command line arguments through `argc` (argument count) and `argv` (argument vector). The program validates that at least one image path was provided, then iterates through all provided paths, processing each image independently.

Example usage:
```bash
./edge_detector img1.png img2.png img3.png
```

This processes all three images, generating `edge_img1.png`, `edge_img2.png`, and `edge_img3.png`.

== Processing the image

The image processing pipeline is encapsulated in the `processImage` function, which handles the complete workflow for a single image:
```c
void processImage(const char* input_path, const char* output_path)
```

Each image goes through these steps independently:

1. Load image from the input path
2. Convert to grayscale
3. Allocate GPU memory
4. Copy data to GPU
5. Launch the Sobel kernel
6. Copy results back to CPU
7. Save to the output path
8. Free all allocated memory

This function isolates the processing of each image. If one image fails to load or process, the program continues with the remaining images rather than terminating completely.

== Automatic Output Naming
```c
char* createOutputFilename(const char* input_path) {
    // Find the last slash or backslash
    const char* filename = strrchr(input_path, '/');
    if (!filename) filename = strrchr(input_path, '\\');
    if (filename) filename++; // Skip the slash
    else filename = input_path; // No path separator found

    // Allocate space for "edge_" + filename
    size_t len = strlen(filename);
    char* output = (char*)malloc(len + 6); // "edge_" = 5 chars + null terminator
    sprintf(output, "edge_%s", filename);

    return output;
}
```

The `createOutputFilename` function generates output filenames by prepending `edge_` to the input filename. The function strips any directory path from the input using `strrchr`, which searches for the last occurrence of a path separator (`/` or `\`).

For example:

```
- `./assets/photo.png` > `edge_photo.png`
- `images/test.png` > `edge_test.png`
- `img.png` > `edge_img.png`
```

All output files are saved in the current directory regardless of where the input images are located.

== Loading the Image
```c
unsigned width, height;
unsigned char* image;
unsigned err = lodepng_decode32_file(&image, &width, &height, input_path);
if (err) {
    printf("Decode Error %u for %s: %s\n", err, input_path, lodepng_error_text(err));
    return 1;
}
```

The program uses the lodepng library to decode PNG files. The `lodepng_decode32_file` function allocates memory and fills it with RGBA pixel data (4 bytes per pixel: red, green, blue, alpha). The width and height variables are filled by reference, returning the image dimensions.

PNG decoding happens entirely on the CPU. The lodepng library handles PNG loading and color space conversion, and gives us raw pixel data ready for processing.

== Grayscale Conversion
```c
size_t num_pixels = width * height;
unsigned char* gray = (unsigned char*)malloc(num_pixels);

for (size_t i = 0; i < num_pixels; i++) {
    unsigned char r = image[i * 4];
    unsigned char g = image[i * 4 + 1];
    unsigned char b = image[i * 4 + 2];
    gray[i] = (r + g + b) / 3;
}

free(image);
```

Edge detection operates on grayscale images, so we convert the RGBA data to grayscale. The conversion averages the red, green, and blue channels, ignoring the alpha channel. The original RGBA image is freed immediately after conversion as we no longer need the color information once grayscale conversion completes.

== GPU Memory Allocation
```c
unsigned char *d_input, *d_output;
size_t bytes = num_pixels * sizeof(unsigned char);

cudaMalloc(&d_input, bytes);
cudaMalloc(&d_output, bytes);
```

The program allocates two buffers on the GPU: `d_input` for the grayscale input and `d_output` for the edge-detected result.

== Kernel Launch
```c
dim3 threads(16, 16);
dim3 blocks(CEIL_DIV(width, 16), CEIL_DIV(height, 16));

sobelKernel<<<blocks, threads>>>(d_input, d_output, width, height);

cudaError_t cuda_err = cudaGetLastError();
if (cuda_err != cudaSuccess) {
    printf("CUDA Error for %s: %s\n", input_path, cudaGetErrorString(cuda_err));
    cudaFree(d_input);
    cudaFree(d_output);
    free(gray);
    return;
}

cudaDeviceSynchronize();
```

== Saving Results
```c
unsigned char* output = (unsigned char*)malloc(num_pixels);
cudaMemcpy(output, d_output, bytes, cudaMemcpyDeviceToHost);

err = lodepng_encode_file(output_path, output, width, height, LCT_GREY, 8);
if (err) {
    printf("Encode Error %u for %s: %s\n", err, output_path, lodepng_error_text(err));
} else {
    printf("Saved: %s\n", output_path);
}
```

After synchronization, the edge-detected result is copied from GPU to CPU memory. The lodepng library then encodes this grayscale data as a PNG file. The `LCT_GREY` color type specifies grayscale encoding, and `8` indicates 8-bit depth `0-255 range`.

== Memory Cleanup
```c
cudaFree(d_input);
cudaFree(d_output);
free(gray);
free(output);
```

GPU memory is freed using `cudaFree`, while CPU memory uses standard `free`.

== Build Commands

To compile the program:
```bash
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t4/src] make
nvcc ./edge_detection.cu ../assets/lodepng.o -o ./edge_detector
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t4/src]
```

To run edge detection on multiple images:
```bash
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t4/src] ./edge_detector ../assets/img.png
Processing 1 image(s)...

Processing: ../assets/img.png (580 x 458)
Saved: edge_img.png

All images processed!
[wizard@archlinux ~/Projects/School/HPC/2431342_SwoyamPokharel_6CS005/t4/src]
```

For multiple images:

```bash
./edge_detector ../assets/img.png ./photos/test.png

 or even,

./edge_detector images/*.png
```

This generates `edge_<input_filename>.png` in the current directory.

To clean build artifacts:
```bash
make clean
```

= Screenshots

#let images = (
  "input.png",
  "output.png",
  "cat.png",
  "edge_cat.png",
  "Screenshot from 2026-01-10 14-28-50.png",
  "Screenshot from 2026-01-10 14-28-52.png",
  "Screenshot from 2026-01-10 14-28-53.png",
  "Screenshot from 2026-01-10 14-28-54.png",
)

#for file in images {
  image("images/screenshots/" + file, width: 100%)
}
